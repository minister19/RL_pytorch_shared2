以下是经过优化和重构后的完整代码，结合了模块化设计、性能优化、可读性提升、错误处理和日志记录等改进点：

---

### **优化后的完整代码**

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import gymnasium as gym
import random
import math
import logging
import unittest


# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


# 优先经验回放缓冲区
class ReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment_per_sampling=0.001, device='cpu'):
        """
        初始化优先经验回放缓冲区。
        :param capacity: 缓冲区的最大容量
        :param alpha: 优先级指数，控制优先级的权重（0 <= alpha <= 1）
        :param beta: 重要性采样权重指数，初始值（0 <= beta <= 1）
        :param beta_increment_per_sampling: 每次采样后 beta 的增量
        :param device: 存储设备（'cpu' 或 'cuda'）
        """
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
        self.alpha = alpha
        self.beta = beta
        self.beta_increment_per_sampling = beta_increment_per_sampling
        self.index = 0
        self.device = device

    def add(self, experience, priority):
        """
        向缓冲区中添加经验。
        :param experience: 经验元组 (state, action, reward, next_state, done)
        :param priority: 该经验的初始优先级
        """
        experience = tuple(map(lambda x: x.to(self.device), experience))
        if len(self.buffer) < self.capacity:
            self.buffer.append(experience)
            self.priorities.append(priority)
        else:
            self.buffer[self.index] = experience
            self.priorities[self.index] = priority
        self.index = (self.index + 1) % self.capacity

    def sample(self, batch_size):
        """
        从缓冲区中采样一批经验。
        :param batch_size: 采样批次大小
        :return: 采样的经验、对应的索引和重要性采样权重
        """
        priorities = np.array(self.priorities)
        probs = priorities ** self.alpha
        probs /= probs.sum()

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[idx] for idx in indices]

        weights = (len(self.buffer) * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        weights = torch.tensor(weights, dtype=torch.float32, device=self.device)

        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])

        return samples, indices, weights

    def update_priorities(self, indices, priorities):
        """
        更新采样经验的优先级。
        :param indices: 需要更新优先级的经验索引
        :param priorities: 新的优先级
        """
        for idx, priority in zip(indices, priorities):
            self.priorities[idx] = priority


# 定义 Transformer 模型
class TransformerModel(nn.Module):
    def __init__(self, input_size, d_model, nhead, num_layers, output_size):
        """
        初始化 Transformer 模型。
        :param input_size: 输入特征的维度
        :param d_model: Transformer 模型的隐藏层维度
        :param nhead: 多头注意力机制的头数
        :param num_layers: Transformer 编码器的层数
        :param output_size: 输出动作的维度
        """
        super(TransformerModel, self).__init__()
        self.embedding = nn.Linear(input_size, d_model)
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, nhead, norm_first=True),
            num_layers
        )
        self.fc = nn.Linear(d_model, output_size)

    def forward(self, x):
        """
        前向传播。
        :param x: 输入状态
        :return: 输出动作值
        """
        x = self.embedding(x)
        x = x.permute(1, 0, 2)
        x = self.transformer_encoder(x)
        x = x[-1, :, :]
        x = self.fc(x)
        return x


# 神经网络模块
class Network:
    def __init__(self, input_size, d_model, nhead, num_layers, output_size, lr, tau, device='cpu'):
        """
        初始化神经网络模块。
        :param input_size: 输入状态的维度
        :param d_model: Transformer 模型的隐藏层维度
        :param nhead: 多头注意力机制的头数
        :param num_layers: Transformer 编码器的层数
        :param output_size: 输出动作的维度
        :param lr: 学习率
        :param tau: 目标网络更新系数
        :param device: 存储设备（'cpu' 或 'cuda'）
        """
        self.device = device
        self.q_net = TransformerModel(input_size, d_model, nhead, num_layers, output_size).to(device)
        self.v_net = TransformerModel(input_size, d_model, nhead, num_layers, output_size).to(device)
        self.v_net.load_state_dict(self.q_net.state_dict())
        self.optimizer = optim.Adam(self.q_net.parameters(), lr=lr)
        self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=100, gamma=0.9)
        self.tau = tau

    def update_target_network(self):
        """
        更新目标网络。
        """
        q_net_state_dict = self.q_net.state_dict()
        v_net_state_dict = self.v_net.state_dict()
        for key in q_net_state_dict:
            v_net_state_dict[key] = self.tau * q_net_state_dict[key] + (1 - self.tau) * v_net_state_dict[key]
        self.v_net.load_state_dict(v_net_state_dict)

    def train(self, batch):
        """
        训练 Q 网络。
        :param batch: 采样的一批经验
        """
        states, actions, rewards, next_states, dones = batch
        q_values = self.q_net(states).gather(1, actions)
        with torch.no_grad():
            target_values = self.v_net(next_states).max(1).values
            target_q_values = rewards + 0.99 * (1 - dones) * target_values.unsqueeze(1)

        loss = nn.functional.mse_loss(q_values, target_q_values)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()


# 探索策略模块
class Exploration:
    def __init__(self, epsi_high, epsi_low, decay, temperature, device='cpu'):
        """
        初始化探索策略模块。
        :param epsi_high: epsilon 的初始值
        :param epsi_low: epsilon 的最小值
        :param decay: epsilon 的衰减率
        :param temperature: 玻尔兹曼探索的温度参数
        :param device: 存储设备（'cpu' 或 'cuda'）
        """
        self.epsi_high = epsi_high
        self.epsi_low = epsi_low
        self.decay = decay
        self.temperature = temperature
        self.steps = 0
        self._epsilon_decay_factor = math.exp(-1.0 / self.decay)
        self.device = device

    def get_epsilon(self):
        """
        计算当前 epsilon 值。
        :return: 当前 epsilon 值
        """
        return self.epsi_low + (self.epsi_high - self.epsi_low) * (self._epsilon_decay_factor ** self.steps)

    def act(self, q_values):
        """
        根据探索策略选择动作。
        :param q_values: Q 网络输出的动作值
        :return: 选择的动作
        """
        self.steps += 1
        epsilon = self.get_epsilon()

        if random.random() > epsilon:
            probabilities = torch.softmax(q_values / self.temperature, dim=1)
            action = torch.multinomial(probabilities, 1)
        else:
            action = torch.tensor([[random.randint(0, q_values.size(1) - 1]], device=self.device)

        return action


# Agent 类
class Agent:
    def __init__(self, env, replay_buffer, network, exploration, gamma, batch_size):
        """
        初始化 Agent。
        :param env: 环境对象
        :param replay_buffer: 经验回放缓冲区
        :param network: 神经网络模块
        :param exploration: 探索策略模块
        :param gamma: 折扣因子
        :param batch_size: 训练批次大小
        """
        self.env = env
        self.replay_buffer = replay_buffer
        self.network = network
        self.exploration = exploration
        self.gamma = gamma
        self.batch_size = batch_size

    def train(self, num_episodes):
        """
        训练 Agent。
        :param num_episodes: 训练的总回合数
        """
        try:
            for episode in range(num_episodes):
                state = self.env.reset()
                state = torch.tensor(state, dtype=torch.float32, device=self.network.device).unsqueeze(0)
                total_reward = 0

                for step in range(1000):
                    q_values = self.network.q_net(state)
                    action = self.exploration.act(q_values)
                    next_state, reward, done, _ = self.env.step(action.item())
                    next_state = torch.tensor(next_state, dtype=torch.float32, device=self.network.device).unsqueeze(0)
                    reward = torch.tensor([reward], dtype=torch.float32, device=self.network.device)
                    done = torch.tensor([done], dtype=torch.int8, device=self.network.device)

                    self.replay_buffer.add((state, action, reward, next_state, done), priority=1.0)
                    state = next_state
                    total_reward += reward.item()

                    if len(self.replay_buffer.buffer) >= self.batch_size:
                        batch, indices, weights = self.replay_buffer.sample(self.batch_size)
                        self.network.train(batch)

                    if done:
                        break

                logging.info(f"Episode {episode + 1}, Total Reward: {total_reward}")
        except Exception as e:
            logging.error(f"An error occurred during training: {e}")


# 单元测试
class TestReplayBuffer(unittest.TestCase):
    def test_add_and_sample(self):
        buffer = ReplayBuffer(capacity=10)
        state = torch.tensor([1.0, 2.0])
        action = torch.tensor([0])
        reward = torch.tensor([1.0])
        next_state = torch.tensor([2.0, 3.0])
        done = torch.tensor([0])
        buffer.add((state, action, reward, next_state, done), priority=1.0)
        samples, indices, weights = buffer.sample(batch_size=1)
        self.assertEqual(len(samples), 1)


if __name__ == '__main__':
    # 初始化环境和超参数
    env = gym.make('CartPole-v1', render_mode="human")
    config = {
        'input_size': env.observation_space.shape[0],
        'd_model': 32,
        'nhead': 4,
        'num_layers': 2,
        'output_size': env.action_space.n,
        'lr': 1e-2,
        'tau': 0.005,
        'gamma': 0.99,
        'epsi_high': 0.9,
        'epsi_low': 0.05,
        'decay': int(1e3),
        'capacity': int(1e4),
        'batch_size': int(1e2),
        'temperature': 1.0,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }

    # 初始化模块
    replay_buffer = ReplayBuffer(capacity=config['capacity'], device=config['device'])
    network = Network(
        input_size=config['input_size'],
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        output_size=config['output_size'],
        lr=config['lr'],
        tau=config['tau'],
        device=config['device']
    )
    exploration = Exploration(
        epsi_high=config['epsi_high'],
        epsi_low=config['epsi_low'],
        decay=config['decay'],
        temperature=config['temperature'],
        device=config['device']
    )

    # 初始化 Agent 并开始训练
    agent = Agent(env, replay_buffer, network, exploration, config['gamma'], config['batch_size'])
    agent.train(num_episodes=1000)
```

---

### **主要改进点总结**

1. **模块化设计**：将代码拆分为 `ReplayBuffer`、`Network`、`Exploration` 和 `Agent` 四个模块，职责清晰。
2. **性能优化**：减少数据传输开销，优化数据结构。
3. **可读性提升**：使用更具描述性的变量名和详细的注释。
4. **错误处理和日志记录**：增强代码的健壮性和可调试性。
5. **单元测试**：为 `ReplayBuffer` 编写单元测试，确保其功能正确。

通过这些优化，代码更易于理解、维护和扩展，同时也为后续的优化和调试打下了良好的基础。

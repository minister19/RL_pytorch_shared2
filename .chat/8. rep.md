下面是一个使用 PyTorch 实现 Transformer 模型的示例代码。这个模型基于论文 "Attention is All You Need" 中的 Transformer 架构，适用于序列到序列的任务（如机器翻译）。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout=0.1):
        super(TransformerModel, self).__init__()

        self.embedding = nn.Embedding(input_dim, d_model)
        self.pos_encoder = PositionalEncoding(d_model, dropout)

        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,
                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)

        self.fc_out = nn.Linear(d_model, output_dim)
        self.d_model = d_model

    def forward(self, src, tgt, src_mask=None, tgt_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None):
        src = self.embedding(src) * math.sqrt(self.d_model)
        src = self.pos_encoder(src)

        tgt = self.embedding(tgt) * math.sqrt(self.d_model)
        tgt = self.pos_encoder(tgt)

        output = self.transformer(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask,
                                 src_key_padding_mask=src_key_padding_mask, tgt_key_padding_mask=tgt_key_padding_mask)

        output = self.fc_out(output)
        return output

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, d_model)
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:x.size(0), :]
        return self.dropout(x)

# 示例用法
input_dim = 1000  # 输入词汇表大小
output_dim = 1000  # 输出词汇表大小
d_model = 512  # 嵌入维度
nhead = 8  # 多头注意力头数
num_encoder_layers = 6  # 编码器层数
num_decoder_layers = 6  # 解码器层数
dim_feedforward = 2048  # 前馈网络维度
dropout = 0.1  # dropout概率

model = TransformerModel(input_dim, output_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)

# 示例输入
src = torch.randint(0, input_dim, (10, 32))  # (序列长度, 批量大小)
tgt = torch.randint(0, output_dim, (20, 32))  # (序列长度, 批量大小)

# 前向传播
output = model(src, tgt)
print(output.shape)  # 输出形状: (tgt_seq_len, batch_size, output_dim)
```

### 代码说明：

1. **TransformerModel**:

   - `input_dim`: 输入词汇表的大小。
   - `output_dim`: 输出词汇表的大小。
   - `d_model`: 嵌入维度。
   - `nhead`: 多头注意力机制的头数。
   - `num_encoder_layers`: 编码器的层数。
   - `num_decoder_layers`: 解码器的层数。
   - `dim_feedforward`: 前馈网络的维度。
   - `dropout`: Dropout 概率。

2. **PositionalEncoding**:

   - 用于向输入序列添加位置编码，以保留序列的顺序信息。

3. **前向传播**:
   - `src`: 源序列（输入）。
   - `tgt`: 目标序列（输出）。
   - `src_mask`, `tgt_mask`: 用于屏蔽源序列和目标序列的掩码。
   - `src_key_padding_mask`, `tgt_key_padding_mask`: 用于屏蔽填充部分的掩码。

### 示例用法：

- `src` 和 `tgt` 是随机生成的输入和目标序列。
- 模型输出形状为 `(tgt_seq_len, batch_size, output_dim)`。

### 注意事项：

- 这个模型是一个基础的 Transformer 实现，适用于序列到序列的任务。你可以根据具体任务调整模型结构。
- 在实际应用中，通常需要添加更多的功能，如学习率调度、损失函数、优化器等。

希望这个示例对你有帮助！
